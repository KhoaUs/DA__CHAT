import streamlit as st
from src.chatbot_app.nlp import get_intent_and_entities
import speech_recognition as sr
from audiorecorder import audiorecorder
from pydub import AudioSegment

# ƒê√¢y l√† kh√¢u "R√ÅP L·∫†I" - Nh√≥m 2 g·ªçi code c·ªßa Nh√≥m 1
# Ban ƒë·∫ßu n√≥ s·∫Ω g·ªçi code "Gi·∫£" 
from src.analysis_core.api import so_sanh_gia, goi_y_san_pham

# --- C·∫•u h√¨nh giao di·ªán ---# st.title(":rainbow[Chatbot Ph√¢n t√≠ch TMƒêT]")
# st.caption(f"ƒê·ªì √°n m√¥n h·ªçc - Ph√¢n t√≠ch d·ªØ li·ªáu th√¥ng minh")

st.set_page_config(
    page_title="Data analysis",
    page_icon=":chart_with_upwards_trend:",
    layout="wide",
)

"""
# :rainbow[:material/query_stats: Data analysis chatbot]

C√¥ng c·ª• ph√¢n t√≠ch d·ªØ li·ªáu th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠

"""

""  # Add some space.

if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "image" in message:
            st.image(message["image"])



def analyse(user_query):
    st.chat_message("user").write(user_query)
    st.session_state.messages.append({"role": "user", "content": user_query})

    # 2. G·ªçi "B·ªô n√£o NLP" (Code c·ªßa Nh√≥m 2)
    intent, entities = get_intent_and_entities(user_query)

    response_text = ""
    response_image = None

    # 3. G·ªçi API (Code c·ªßa Nh√≥m 1)
    try:
        if intent == "so_sanh_gia":
            result = so_sanh_gia(entities.get("ten_san_pham"))
        
            # Format k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ API
            response_text = f"K·∫øt qu·∫£ so s√°nh gi√° cho '{entities.get('ten_san_pham')}':\n"
            for item in result.get("data", []):
                response_text += f"- {item['platform']}: {item['price']:,} VND\n"

            # L·∫•y bi·ªÉu ƒë·ªì (theo y√™u c·∫ßu ƒë·ªÅ t√†i)
            response_image = result.get("chart_path")

        elif intent == "goi_y_san_pham":
            result = goi_y_san_pham(entities.get("danh_muc"), entities.get("gia_max"))

            response_text = "ƒê√¢y l√† c√°c g·ª£i √Ω cho b·∫°n:\n"
            for item in result.get("data", []):
                response_text += f"- {item['name']} (Gi√°: {item['price']:,} VND)\n"

        else: # (intent == "fallback")
            response_text = "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu y√™u c·∫ßu c·ªßa b·∫°n. T√¥i ch·ªâ c√≥ th·ªÉ (1) so s√°nh gi√° ho·∫∑c (2) g·ª£i √Ω s·∫£n ph·∫©m."


    except Exception as e:
        response_text = f"ƒê√£ x·∫£y ra l·ªói: {e}"

    # 4. Hi·ªÉn th·ªã k·∫øt qu·∫£ c·ªßa Bot
    with st.chat_message("assistant"):
        st.write(response_text)
        if response_image:
            st.image(response_image)

    # L∆∞u tin nh·∫Øn c·ªßa bot
    st.session_state.messages.append({
        "role": "assistant",
        "content": response_text,
        "image": response_image
    })



# --- Chat input bar with mic ---
col1, col2 = st.columns([10, 1])

with col1:
    user_query = st.chat_input("B·∫°n c·∫ßn t√¥i ph√¢n t√≠ch g√¨?")

with col2:
    # The built-in microphone input
    audio_file = audiorecorder("üéôÔ∏è", "üî¥üéôÔ∏è")

# --- Handle typed message ---
if user_query:
    analyse(user_query)

# --- Handle audio input ---
if audio_file:
    # Play audio
    st.audio(audio_file.export().read(), format="audio/wav")

    # Export audio to wav
    audio_file.export("audio.wav", format="wav")

    # Transcribe the audio
    recognizer = sr.Recognizer()
    with sr.AudioFile('audio.wav') as source:
        audio_data = recognizer.record(source)

    try:
        text = recognizer.recognize_google(audio_data, language="vi-VN")
        analyse(text)  # Feed into chatbot
    except sr.UnknownValueError:
        st.warning("Kh√¥ng th·ªÉ nh·∫≠n di·ªán gi·ªçng n√≥i, vui l√≤ng th·ª≠ l·∫°i.")
    except sr.RequestError as e:
        st.error(f"L·ªói k·∫øt n·ªëi t·ªõi d·ªãch v·ª• nh·∫≠n di·ªán gi·ªçng n√≥i: {e}")
